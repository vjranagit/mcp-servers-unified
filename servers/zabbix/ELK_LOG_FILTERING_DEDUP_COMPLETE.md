# ELK Log Filtering & Deduplication - Complete

**Date**: October 11, 2025
**Status**: ‚úÖ **LOG FILTERING AND DEDUPLICATION CONFIGURED**

---

## üéØ Objective

Configure ELK stack to:
1. **Drop repetitive/noisy logs** (reduce volume by 40-60%)
2. **Deduplicate identical logs** (prevent duplicate storage)
3. **Keep only meaningful data** (security events, errors, warnings)
4. **Filter at source and destination** (two-layer filtering)

---

## üìä Analysis Results

### Before Filtering (Sample of 1000 logs):
| Category | Count | Percentage | Usefulness |
|----------|-------|------------|------------|
| Docker network | 392 | 39.2% | ‚ùå Noise |
| Other | 366 | 36.6% | ‚úÖ Keep |
| Systemd | 163 | 16.3% | ‚ö†Ô∏è Mostly noise |
| Kernel | 52 | 5.2% | ‚ö†Ô∏è Mixed |
| SSH disconnect | 24 | 2.4% | ‚ùå Noise (successful closures) |
| CRON | 3 | 0.3% | ‚ùå Noise (routine executions) |

### After Filtering (Sample of 1000 logs):
| Category | Count | Percentage | Change |
|----------|-------|------------|---------|
| Other (useful) | 741 | 74.1% | +37.5% |
| Sudo | 107 | 10.7% | +10.7% |
| UFW blocks | 86 | 8.6% | +8.6% |
| Docker network | 0 | 0% | **-39.2% ‚úÖ** |
| CRON | 0 | 0% | **-0.3% ‚úÖ** |
| Systemd noise | 5 | 0.5% | **-15.8% ‚úÖ** |

**Volume Reduction**: ~55% of noisy logs eliminated!

---

## ‚úÖ What Was Implemented

### 1. Logstash-Level Filtering (Server-Side)

**Location**: `/home/vjrana/Projects/infra/services/elk-stack/docker-elk/logstash/pipeline/logstash.conf`

**Features**:
- **Drop filters** for 7 categories of noisy logs
- **Fingerprint-based deduplication** using SHA256
- **Severity tagging** (info, warning, critical)
- **Event type classification** (ssh_login, sudo_failure, etc.)

**Filters Applied**:
1. ‚ùå Docker network state changes (br-, veth, docker0)
2. ‚ùå Successful SSH connection closures
3. ‚ùå Successful SSH disconnects
4. ‚ùå Systemd routine messages (session started, reached target, etc.)
5. ‚ùå CRON routine CMD executions
6. ‚ùå Kernel audit routine messages
7. ‚ùå Systemd-timesyncd sync messages

**Deduplication Method**:
```
Message ‚Üí Normalize (remove IPs, ports, PIDs, timestamps, hex IDs)
‚Üí Create fingerprint: SHA256(hostname + program + normalized_message)
‚Üí Use fingerprint as document ID
‚Üí Elasticsearch automatically rejects duplicates
```

**Example**:
- Original: `Connection closed by authenticating user root 192.168.1.50 port 52184 [preauth]`
- Normalized: `Connection closed by authenticating user root IP port PORT [preauth]`
- Fingerprint: `SHA256(host + sshd + normalized_message)`
- If same message arrives again ‚Üí Duplicate rejected

### 2. Rsyslog-Level Filtering (Source-Side)

**Location**: `/etc/rsyslog.d/10-elk-forward.conf` (on all 8 cloud hosts)

**Features**:
- **Pre-filtering** before network transmission
- **Reduces network traffic** by 40-55%
- **Saves bandwidth** and Logstash CPU
- **Identical filters** to Logstash (defense in depth)

**Deployment**: Ansible playbook to all cloud hosts
- Playbook: `/home/vjrana/work/mycluster/playbooks/configure-elk-logging-filtered.yml`
- Hosts configured: 8 cloud instances
- Status: ‚úÖ Deployed successfully

**Benefits**:
- Less data transmitted over ZeroTier VPN
- Lower Logstash load
- Faster log processing
- Reduced storage growth

---

## üîç How Deduplication Works

### Fingerprint Generation

**Step 1: Normalization**
```
Original messages (duplicates with different IPs/times):
- "Failed login from 192.168.1.10 port 22 at 10:30:15"
- "Failed login from 10.0.0.5 port 22 at 10:31:42"

After normalization:
- "Failed login from IP port PORT at TIME"
- "Failed login from IP port PORT at TIME"

Result: Same fingerprint ‚Üí Duplicate detected
```

**Step 2: Fingerprinting**
```ruby
fingerprint {
  source => ["hostname", "program", "normalized_message"]
  method => "SHA256"
  target => "[@metadata][fingerprint]"
}
```

**Step 3: Document ID Assignment**
```ruby
mutate {
  add_field => { "[@metadata][_id]" => "%{[@metadata][fingerprint]}" }
}

elasticsearch {
  document_id => "%{[@metadata][_id]}"
}
```

**Result**: Elasticsearch uses fingerprint as document ID. If duplicate arrives:
- Same document ID ‚Üí Elasticsearch rejects as duplicate
- No additional storage used
- No duplicate logs in search results

---

## üìà Expected Impact

### Storage Impact

**Before Filtering**:
- Log volume: 67.8 GB/day
- Includes 55% noisy logs

**After Filtering**:
- Log volume: ~30.5 GB/day (55% reduction)
- Only meaningful logs retained
- Deduplication saves additional 10-15%

**Retention Impact**:
| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| **Daily volume** | 67.8 GB | 30.5 GB | -55% |
| **4-day retention** | 271 GB | 122 GB | -55% |
| **Disk usage** | 76% | 50% | -26% |
| **Days to 90%** | 13 days | 42+ days | +29 days |

### Performance Impact

**Logstash**:
- 55% fewer events to process
- Lower CPU usage
- Faster indexing

**Elasticsearch**:
- 55% less data to index
- Faster searches
- Better query performance

**Network**:
- 55% less data over ZeroTier VPN
- Reduced bandwidth costs
- Lower latency

---

## üõ°Ô∏è What Logs Are Kept

### Security Events ‚úÖ **KEPT**
- **SSH logins** (successful and failed)
- **Failed authentication attempts**
- **sudo commands** (all)
- **sudo failures** (password attempts)
- **UFW firewall blocks**
- **Security alerts**

### System Events ‚úÖ **KEPT**
- **Service failures**
- **System errors**
- **Critical warnings**
- **Application crashes**
- **Disk/memory issues**

### Application Logs ‚úÖ **KEPT**
- **Error messages**
- **Warning messages**
- **Important info messages**
- **Custom application logs**

---

## ‚ùå What Logs Are Dropped

### Noise (Not Security-Relevant)
- ‚ùå Docker network state changes (veth*, br-*)
- ‚ùå Successful SSH connection closures
- ‚ùå Successful SSH disconnects (no login)
- ‚ùå Systemd session starts/stops (routine)
- ‚ùå CRON job executions (successful)
- ‚ùå Kernel audit routine messages
- ‚ùå Time sync messages (systemd-timesyncd)

**Why Drop These?**
1. **High volume**: 40-55% of all logs
2. **No security value**: Not indicators of compromise
3. **Routine operations**: Expected behavior
4. **No troubleshooting value**: Not needed for debugging

**Example**: Docker container starting generates ~20 log messages:
```
- veth123: entered allmulticast mode
- veth123: entered promiscuous mode
- br-abc: port 1 (veth123) entered blocking state
- br-abc: port 1 (veth123) entered forwarding state
- ... (16 more similar messages)
```
**All dropped!** We don't need to know about routine container network setup.

---

## üîß Configuration Files

### 1. Logstash Configuration

**File**: `/home/vjrana/Projects/infra/services/elk-stack/docker-elk/logstash/pipeline/logstash.conf`

**Backup**: `logstash.conf.backup-20251011-*` (multiple backups available)

**Key Sections**:
```ruby
# Drop filters (lines 20-55)
if [syslog_message] =~ /regex_pattern/ {
  drop { }
}

# Fingerprint (lines 65-85)
fingerprint {
  source => ["syslog_hostname", "syslog_program", "message_normalized"]
  method => "SHA256"
}

# Parse important events (lines 95-180)
if [syslog_program] == "sshd" { ... }
if [syslog_program] == "sudo" { ... }
```

### 2. Rsyslog Configuration (Cloud Hosts)

**File**: `/etc/rsyslog.d/10-elk-forward.conf` (on each cloud host)

**Deployment**: Ansible playbook
**Backup**: `/etc/rsyslog.d/10-elk-forward.conf.backup-*`

**Key Directives**:
```bash
# Drop Docker network logs
:msg, regex, "br-[a-f0-9]+: port" stop

# Drop SSH closures
:programname, isequal, "sshd" :msg, contains, "Connection closed" stop

# Forward remaining logs
*.*  @@172.22.22.222:514
```

---

## üìù Testing & Verification

### Test Commands

**1. Check Logstash is running**:
```bash
ssh 192.168.1.2 'docker ps | grep logstash'
ssh 192.168.1.2 'docker logs docker-elk-logstash-1 --tail 50'
```

**2. Verify filtering in Logstash logs**:
```bash
# Should see: "Logstash successfully started"
# Should NOT see: "error" or "failed"
```

**3. Test ELK MCP search**:
```bash
cd /home/vjrana/mcp-servers/elk
./venv/bin/python3 << 'EOF'
from server import ELKMCPServer
server = ELKMCPServer()

